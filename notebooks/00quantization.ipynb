{
 "metadata": {
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "metadata": {
   "interpreter": {
    "hash": "3503e998b5c3fd447f3e41d7b6c27225a8aa2bff77cd3ce83957dd0c73daee22"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import os\n",
    "import time\n",
    "import sys\n",
    "import torch.quantization\n",
    "from pytorch_lightning import Trainer\n",
    "import os\n",
    "os.chdir(\"/home/k.schwienbacher/quantization-robustness\")\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from src.models.simple import SimpleModel\n",
    "from src.models.lenet import LeNet\n",
    "from src.models.resnet import ResNet18\n",
    "from src.utils.model import load_experiment\n",
    "from src.experiments.pruning import get_prunable_modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nonzero(tensor):\n",
    "    \"\"\"Returns absolute number of values different from 0\n",
    "    Arguments:\n",
    "        tensor {numpy.ndarray} -- Array to compute over\n",
    "    Returns:\n",
    "        int -- Number of nonzero elements\n",
    "    \"\"\"\n",
    "    return np.sum(tensor != 0.0)\n",
    "\n",
    "\n",
    "# https://pytorch.org/docs/stable/tensor_attributes.html\n",
    "dtype2bits = {\n",
    "    torch.float32: 32,\n",
    "    torch.float: 32,\n",
    "    torch.float64: 64,\n",
    "    torch.double: 64,\n",
    "    torch.float16: 16,\n",
    "    torch.half: 16,\n",
    "    torch.uint8: 8,\n",
    "    torch.int8: 8,\n",
    "    torch.int16: 16,\n",
    "    torch.short: 16,\n",
    "    torch.int32: 32,\n",
    "    torch.int: 32,\n",
    "    torch.int64: 64,\n",
    "    torch.long: 64,\n",
    "    torch.bool: 1,\n",
    "}\n",
    "\n",
    "def model_size(model, as_bits=False):\n",
    "    \"\"\"Returns absolute and nonzero model size\n",
    "    Arguments:\n",
    "        model {torch.nn.Module} -- Network to compute model size over\n",
    "    Keyword Arguments:\n",
    "        as_bits {bool} -- Whether to account for the size of dtype\n",
    "    Returns:\n",
    "        int -- Total number of weight & bias params\n",
    "        int -- Out total_params exactly how many are nonzero\n",
    "    \"\"\"\n",
    "\n",
    "    total_params = 0\n",
    "    nonzero_params = 0\n",
    "    for tensor in model.parameters():\n",
    "        t = np.prod(tensor.shape)\n",
    "        nz = nonzero(tensor.detach().cpu().numpy())\n",
    "        if as_bits:\n",
    "            bits = dtype2bits[tensor.dtype]\n",
    "            t *= bits\n",
    "            nz *= bits\n",
    "        total_params += t\n",
    "        nonzero_params += nz\n",
    "    return int(total_params), int(nonzero_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LeNet.load_from_checkpoint('/data/logs/kristian/runs/default/07-27-19/df1f105b-7584-4f48-b4d1-e7b03e27fefd/model_0.03125.ckpt')\n",
    "model = ResNet18.load_from_checkpoint('/data/logs/kristian/runs/default/14-22-16/e11c17f0-1336-40cd-aedb-697485c51f89/model_0.03125.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "model05 = ResNet18.load_from_checkpoint('/data/logs/kristian/runs/default/14-22-16/e11c17f0-1336-40cd-aedb-697485c51f89/model_0.5.ckpt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4999995528384829\n",
      "0.4999995528384829\n",
      "0.4999995528384829\n",
      "0.4999995528384829\n",
      "0.4999995528384829\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "model = ResNet18()\n",
    "\n",
    "for i in range(5):\n",
    "    modules = get_prunable_modules(model.model)\n",
    "\n",
    "    for module in modules:\n",
    "        torch.nn.utils.prune.l1_unstructured(module, name=\"weight\", amount=0.5)\n",
    "    \n",
    "   \n",
    "\n",
    "    pruned_model = copy.deepcopy(model)\n",
    "    modules = get_prunable_modules(pruned_model.model)\n",
    "    for module in modules:\n",
    "        torch.nn.utils.prune.remove(module, \"weight\")\n",
    "    tot, nonz = model_size(model)\n",
    "    print((tot-nonz) / tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "def evaluate(model, criterion, data_loader, neval_batches):\n",
    "    model.eval()\n",
    "    top1 = AverageMeter('Acc@1', ':6.2f')\n",
    "    top5 = AverageMeter('Acc@5', ':6.2f')\n",
    "    cnt = 0\n",
    "    with torch.no_grad():\n",
    "        for image, target in data_loader:\n",
    "            output = model(image)\n",
    "            loss = criterion(output, target)\n",
    "            cnt += 1\n",
    "            acc1, acc5 = accuracy(output, target, topk=(1, 5))\n",
    "            print('.', end = '')\n",
    "            top1.update(acc1[0], image.size(0))\n",
    "            top5.update(acc5[0], image.size(0))\n",
    "            if cnt >= neval_batches:\n",
    "                 return top1, top5\n",
    "\n",
    "    return top1, top5\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self, name, fmt=':f'):\n",
    "        self.name = name\n",
    "        self.fmt = fmt\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "    def __str__(self):\n",
    "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
    "        return fmtstr.format(**self.__dict__)\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].reshape(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (quant): QuantStub()\n",
       "  (lin1): Linear(in_features=784, out_features=256, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (lin2): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (dequant): DeQuantStub()\n",
       "  (criterion): CrossEntropyLoss()\n",
       "  (train_accuracy): Accuracy()\n",
       "  (val_accuracy): Accuracy()\n",
       "  (test_accuracy): Accuracy()\n",
       ")"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "path = '/home/kredde/uni/quantization/logs/runs/2021-04-11/12-58-36'\n",
    "num_eval_batches=1000\n",
    "eval_batches_size=64\n",
    "\n",
    "model, datamodule = load_experiment(path)\n",
    "test_dataloader = datamodule.test_dataloader()\n",
    "train_dataloader = datamodule.train_dataloader()\n",
    "\n",
    "trainer = Trainer()\n",
    "float_model = model\n",
    "float_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "float_model.eval()\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size of baseline model\n",
      "Size (MB): 0.816019\n",
      "............................................................................................................................................................."
     ]
    }
   ],
   "source": [
    "print(\"Size of baseline model\")\n",
    "size = print_size_of_model(float_model)\n",
    "top1, top5 = evaluate(float_model, criterion, test_dataloader, neval_batches=num_eval_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Evaluation accuracy on 64000 images, 6.98\n"
     ]
    }
   ],
   "source": [
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batches_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 112.95it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test/loss': 2.3150315284729004, 'test/acc': 0.0697999969124794}]"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "trainer.test(float_model, test_dataloaders=[test_dataloader], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "myModel, datamodule = load_experiment(path)\n",
    "myModel.eval()\n",
    "\n",
    "# Fuse Conv, bn and relu\n",
    "myModel.fuse_model()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "QConfig(activation=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, reduce_range=True), weight=functools.partial(<class 'torch.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric))\n",
      "Post Training Quantization Prepare: Inserting Observers\n",
      "................................Post Training Quantization: Calibration done\n",
      "Post Training Quantization: Convert done\n",
      "Size of model after quantization\n",
      "Size (MB): 0.208201\n",
      ".............................................................................................................................................................Evaluation accuracy on 64000 images, 7.64\n"
     ]
    }
   ],
   "source": [
    "num_calibration_batches = 32\n",
    "\n",
    "# Specify quantization configuration\n",
    "# Start with simple min/max range estimation and per-tensor quantization of weights\n",
    "myModel.qconfig = torch.quantization.default_qconfig\n",
    "print(myModel.qconfig)\n",
    "torch.quantization.prepare(myModel, inplace=True)\n",
    "\n",
    "# Calibrate first\n",
    "print('Post Training Quantization Prepare: Inserting Observers')\n",
    "# print('\\n Inverted Residual Block:After observer insertion \\n\\n', myModel.features[1].conv)\n",
    "\n",
    "# Calibrate with the training set\n",
    "evaluate(myModel, criterion, train_dataloader, neval_batches=num_calibration_batches)\n",
    "print('Post Training Quantization: Calibration done')\n",
    "\n",
    "# Convert to quantized model\n",
    "torch.quantization.convert(myModel, inplace=True)\n",
    "print('Post Training Quantization: Convert done')\n",
    "# print('\\n Inverted Residual Block: After fusion and quantization, note fused modules: \\n\\n',myModel.features[1].conv)\n",
    "\n",
    "print(\"Size of model after quantization\")\n",
    "print_size_of_model(myModel)\n",
    "\n",
    "top1, top5 = evaluate(myModel, criterion, test_dataloader, neval_batches=num_eval_batches)\n",
    "print('Evaluation accuracy on %d images, %2.2f'%(num_eval_batches * eval_batches_size, top1.avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Size (MB): 0.208201\n"
     ]
    }
   ],
   "source": [
    "print_size_of_model(myModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Testing: 100%|██████████| 157/157 [00:01<00:00, 112.61it/s]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[{'test/loss': 2.3441805839538574, 'test/acc': 0.07519999891519547}]"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "test = trainer.test(myModel, test_dataloaders=[test_dataloader], verbose=False)\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}